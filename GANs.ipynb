{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Different GANs implementations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model and Dataset Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a model and dataset and execute all cells of the notebook. Generated images during training will be placed in `./images/<model-name>/<dataset-name>/`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Takes noise vectors as input -- #\n",
    "\n",
    "# model = \"gan\"\n",
    "# model = \"dcgan\"\n",
    "\n",
    "# -- Takes images as input -- #\n",
    "\n",
    "# model = \"fdcgan\"\n",
    "model = \"resnet\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Selection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Only adapted to noise vectors input -- #\n",
    "\n",
    "# dataset = \"mnist\"\n",
    "\n",
    "# -- Adapted to both noise vectors and images input -- #\n",
    "\n",
    "# dataset = \"apple2orange64\"\n",
    "dataset = \"orange2apple64\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import math\n",
    "import itertools\n",
    "import datetime\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.utils import save_image, make_grid\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters and Datasets Specific Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Global Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0         # epoch to start training from\n",
    "n_epochs = 200          # number of epochs of training\n",
    "decay_epoch = 100       # epoch from which to start lr decay\n",
    "n_cpu = 8               # number of cpu threads to use during batch generation\n",
    "lr = 0.0002             # adam: learning rate\n",
    "b1 = 0.5                # adam: decay of first order momentum of gradient\n",
    "b2 = 0.999              # adam: decay of first order momentum of gradient\n",
    "sample_interval = 400   # interval between image samples\n",
    "checkpoint_interval = 1 # interval between batches for saving model checkpoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Datasets Specifications**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"mnist\":\n",
    "    batch_size = 64     # size of the batches\n",
    "    img_size = 28       # size of each image dimension\n",
    "    latent_dim = 100    # dimensionality of the latent space\n",
    "    channels = 1        # grayscale\n",
    "    \n",
    "elif dataset == \"apple2orange64\" or dataset == \"orange2apple64\":\n",
    "    batch_size = 4      # size of the batches\n",
    "    img_size = 64       # size of each image dimension\n",
    "    latent_dim = 300    # dimensionality of the latent space\n",
    "    channels = 3        # rgb\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Unknown dataset\")\n",
    "\n",
    "print({\n",
    "    \"n_epochs\": n_epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"lr\": lr,\n",
    "    \"b1\": b1,\n",
    "    \"b2\": b2,\n",
    "    \"n_cpu\": n_cpu,\n",
    "    \"latent_dim\": latent_dim,\n",
    "    \"img_size\": img_size,\n",
    "    \"channels\": channels,\n",
    "    \"sample_interval\": sample_interval\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Chosen Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == \"gan\":\n",
    "    from gan import Generator, Discriminator\n",
    "    \n",
    "elif model == \"dcgan\":\n",
    "    from dcgan import Generator, Discriminator, weights_init_normal\n",
    "    if dataset == \"mnist\":\n",
    "        img_size = 32\n",
    "\n",
    "elif model == \"fdcgan\" or model == \"resnet\":\n",
    "    lambda_id = 5.0\n",
    "    if dataset not in [\"apple2orange64\", \"orange2apple64\"]:\n",
    "        raise Exception(f\"Dataset {dataset} has no input image for the generator\")\n",
    "    if model == \"fdcgan\":\n",
    "        from fdcgan import Generator, Discriminator, weights_init_normal\n",
    "    elif model == \"resnet\":\n",
    "        from resnet import Generator, Discriminator, weights_init_normal\n",
    "        n_residual_blocks = 9\n",
    "\n",
    "else:\n",
    "    raise Exception(\"Unknown model\")\n",
    "\n",
    "img_shape = (channels, img_size, img_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create Progress and Checkpoint Directories**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_progress_folder = \"images/%s/%s/\" % (model, dataset)\n",
    "\n",
    "os.makedirs(image_progress_folder, exist_ok=True)\n",
    "os.makedirs(\"./saved_models/%s/%s\" % (model, dataset), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Setting Up Cuda**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = True if torch.cuda.is_available() else False\n",
    "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
    "print(\"Device use for training: \", device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialising Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Loss functions -- #\n",
    "\n",
    "if model == \"gan\" or model == \"dcgan\" or model == \"fdcgan\":\n",
    "    adversarial_loss = torch.nn.BCELoss()\n",
    "elif model == \"resnet\":\n",
    "    adversarial_loss = torch.nn.MSELoss()\n",
    "\n",
    "identity_loss = torch.nn.L1Loss()\n",
    "\n",
    "# -- Initialize generator and discriminator -- #\n",
    "\n",
    "if model == \"gan\" or model == \"dcgan\":\n",
    "    generator = Generator(img_shape, latent_dim)\n",
    "elif model == \"fdcgan\":\n",
    "    generator = Generator(img_shape)\n",
    "elif model == \"resnet\":\n",
    "    generator = Generator(img_shape, n_residual_blocks)\n",
    "\n",
    "discriminator = Discriminator(img_shape)\n",
    "\n",
    "if cuda:\n",
    "    generator.cuda()\n",
    "    discriminator.cuda()\n",
    "    adversarial_loss.cuda()\n",
    "    identity_loss.cuda()\n",
    "\n",
    "if start_epoch != 0:\n",
    "    # Load pretrained models\n",
    "    generator.load_state_dict(torch.load(\"saved_models/%s/%s/generator_%d.pth\" % (model, dataset, start_epoch)))\n",
    "    discriminator.load_state_dict(torch.load(\"saved_models/%s/%s/discriminator_%d.pth\" % (model, dataset, start_epoch)))\n",
    "else:\n",
    "    # Initialize weights\n",
    "    if model == \"dcgan\" or model == \"fdcgan\"  or model == \"resnet\":\n",
    "        generator.apply(weights_init_normal)\n",
    "        discriminator.apply(weights_init_normal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LambdaLR:\n",
    "    def __init__(self, n_epochs, offset, decay_start_epoch):\n",
    "        assert (n_epochs - decay_start_epoch) > 0, \"Decay must start before the training session ends!\"\n",
    "        self.n_epochs = n_epochs\n",
    "        self.offset = offset\n",
    "        self.decay_start_epoch = decay_start_epoch\n",
    "\n",
    "    def step(self, epoch):\n",
    "        return 1.0 - max(0, epoch + self.offset - self.decay_start_epoch) / (self.n_epochs - self.decay_start_epoch)\n",
    "    \n",
    "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
    "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
    "\n",
    "# Learning rate update schedulers\n",
    "lr_scheduler_G = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_G, lr_lambda=LambdaLR(n_epochs, start_epoch, decay_epoch).step\n",
    ")\n",
    "lr_scheduler_D = torch.optim.lr_scheduler.LambdaLR(\n",
    "    optimizer_D, lr_lambda=LambdaLR(n_epochs, start_epoch, decay_epoch).step\n",
    ")\n",
    "\n",
    "Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_rgb(image):\n",
    "    rgb_image = Image.new(\"RGB\", image.size)\n",
    "    rgb_image.paste(image)\n",
    "    return rgb_image\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, root, transforms_=None, unaligned=False, mode=\"train\"):\n",
    "        self.transform = transforms.Compose(transforms_)\n",
    "        self.unaligned = unaligned\n",
    "\n",
    "        self.files_A = sorted(glob.glob(os.path.join(root, \"%s/A\" % mode) + \"/*.*\"))\n",
    "        self.files_B = sorted(glob.glob(os.path.join(root, \"%s/B\" % mode) + \"/*.*\"))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image_A = Image.open(self.files_A[index % len(self.files_A)])\n",
    "\n",
    "        if self.unaligned:\n",
    "            image_B = Image.open(self.files_B[random.randint(0, len(self.files_B) - 1)])\n",
    "        else:\n",
    "            image_B = Image.open(self.files_B[index % len(self.files_B)])\n",
    "\n",
    "        # Convert grayscale images to rgb\n",
    "        if image_A.mode != \"RGB\":\n",
    "            image_A = to_rgb(image_A)\n",
    "        if image_B.mode != \"RGB\":\n",
    "            image_B = to_rgb(image_B)\n",
    "\n",
    "        item_A = self.transform(image_A)\n",
    "        item_B = self.transform(image_B)\n",
    "        return {\"A\": item_A, \"B\": item_B}\n",
    "\n",
    "    def __len__(self):\n",
    "        return max(len(self.files_A), len(self.files_B))\n",
    "\n",
    "transforms_ = [\n",
    "    transforms.Resize(int(img_size * 1.12), Image.BICUBIC),\n",
    "    transforms.RandomCrop((img_size, img_size)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "]\n",
    "\n",
    "if dataset == \"mnist\":\n",
    "    \n",
    "    os.makedirs(\"./datasets/mnist\", exist_ok=True)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        datasets.MNIST(\n",
    "            \"./datasets/mnist\",\n",
    "            train=True,\n",
    "            download=True,\n",
    "            transform=transforms.Compose(\n",
    "                [transforms.Resize(img_size), transforms.ToTensor(), transforms.Normalize([0.5], [0.5])]\n",
    "            ),\n",
    "        ),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=n_cpu,\n",
    "    )\n",
    "\n",
    "if dataset == \"apple2orange64\":\n",
    "    my_class_A = \"A\"\n",
    "    my_class_B = \"B\"\n",
    "\n",
    "if dataset == \"orange2apple64\":\n",
    "    my_class_A = \"B\"\n",
    "    my_class_B = \"A\"\n",
    "\n",
    "if dataset == \"apple2orange64\" or dataset == \"orange2apple64\":\n",
    "\n",
    "    import subprocess\n",
    "    command = \"bash ./datasets/download_cyclegan_dataset.sh\"\n",
    "    subprocess.run(command, shell=True)\n",
    "\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        ImageDataset(\"./datasets/apple2orange64\", transforms_=transforms_, unaligned=True, mode=\"train\"),\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=n_cpu,\n",
    "    )\n",
    "\n",
    "    # Test data loader\n",
    "    val_dataloader = DataLoader(\n",
    "        ImageDataset(\"./datasets/apple2orange64\", transforms_=transforms_, unaligned=True, mode=\"validation\"),\n",
    "        batch_size=5,\n",
    "        shuffle=True,\n",
    "        num_workers=1,\n",
    "    )\n",
    "\n",
    "def sample_images(batches_done, my_class_A):\n",
    "    \"\"\"Saves a generated sample from the test set\"\"\"\n",
    "    imgs = next(iter(val_dataloader))\n",
    "    generator.eval()\n",
    "    real = Variable(imgs[my_class_A].type(Tensor))\n",
    "    fake = generator(real)\n",
    "    # Arange images along x-axis\n",
    "    real = make_grid(real, nrow=5, normalize=True)\n",
    "    fake = make_grid(fake, nrow=5, normalize=True)\n",
    "    # Arange images along y-axis\n",
    "    image_grid = torch.cat((real, fake), 1)\n",
    "    save_image(image_grid, \"images/%s/%s/%s.png\" % (model, dataset, batches_done), normalize=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately depending on how the dataset is imported we need to do separated training loops for mnist and apple2orange but the procedure is exactly the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Using '{model}' with '{dataset}', saving progress to '{image_progress_folder}'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random vector noise input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**mnist:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if dataset == \"mnist\":\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(n_epochs):\n",
    "        for i, (imgs, _) in enumerate(dataloader):\n",
    "\n",
    "            if (imgs.shape[0] != batch_size):\n",
    "                continue\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(imgs.shape[0], 1).fill_(1.0), requires_grad=False)\n",
    "            fake = Variable(Tensor(imgs.shape[0], 1).fill_(0.0), requires_grad=False)\n",
    "\n",
    "            # Configure input\n",
    "            real_imgs = Variable(imgs.type(Tensor))\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z)\n",
    "\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            loss_identity = identity_loss(gen_imgs, real_imgs)\n",
    "\n",
    "            loss_GAN = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "            # Total loss\n",
    "            g_loss = loss_GAN + lambda_id * loss_identity\n",
    "\n",
    "            g_loss.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            batches_left = n_epochs * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "            \n",
    "            # Print log\n",
    "            sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] ETA: %s\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item(), time_left))\n",
    "\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            if batches_done % sample_interval == 0:\n",
    "                save_image(gen_imgs.data[:5], image_progress_folder + \"/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "            \n",
    "        if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "            # Save model checkpoints\n",
    "            torch.save(generator.state_dict(), \"saved_models/%s/%s/generator_%d.pth\" % (model, dataset, epoch))\n",
    "            torch.save(discriminator.state_dict(), \"saved_models/%s/%s/generator_%d.pth\" % (model, dataset, epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**orange2apple or apple2orange with gan or dcgan:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset == \"orange2apple64\" or dataset == \"apple2orange64\") and (model == \"gan\" or model == \"dcgan\"):\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            # Set model input\n",
    "\n",
    "            if dataset == \"orange2apple64\" or dataset == \"apple2orange64\":\n",
    "                real_imgs = Variable(batch[my_class_B].type(Tensor))\n",
    "\n",
    "                if (real_imgs.shape[0] != batch_size):\n",
    "                    continue\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((real_imgs.size(0), 1))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_imgs.size(0), 1))), requires_grad=False)\n",
    "\n",
    "            # -----------------\n",
    "            #  Train Generator\n",
    "            # -----------------\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Sample noise as generator input\n",
    "            z = Variable(Tensor(np.random.normal(0, 1, (real_imgs.shape[0], latent_dim))))\n",
    "\n",
    "            # Generate a batch of images\n",
    "            gen_imgs = generator(z)\n",
    "            \n",
    "            # Identity loss, Generator should be identity if real image is fed\n",
    "            loss_identity = identity_loss(generator(real_imgs), real_imgs) \n",
    "\n",
    "            # Loss measures generator's ability to fool the discriminator\n",
    "            loss_GAN = adversarial_loss(discriminator(gen_imgs), valid)\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_id * loss_identity\n",
    "\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Measure discriminator's ability to classify real from generated samples\n",
    "            real_loss = adversarial_loss(discriminator(real_imgs), valid)\n",
    "            fake_loss = adversarial_loss(discriminator(gen_imgs.detach()), fake)\n",
    "            d_loss = (real_loss + fake_loss) / 2\n",
    "\n",
    "            d_loss.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "            \n",
    "            # Determine approximate time left\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            batches_left = n_epochs * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "\n",
    "            sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f] ETA: %s\"\n",
    "                % (epoch, n_epochs, i, len(dataloader), d_loss.item(), g_loss.item(), time_left))\n",
    "\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            if batches_done % sample_interval == 0:\n",
    "                save_image(gen_imgs.data[:batch_size], image_progress_folder + \"/%d.png\" % batches_done, nrow=5, normalize=True)\n",
    "        \n",
    "        if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "            # Save model checkpoints\n",
    "            torch.save(generator.state_dict(), \"saved_models/%s/%s/generator_%d.pth\" % (model, dataset, epoch))\n",
    "            torch.save(discriminator.state_dict(), \"saved_models/%s/%s/generator_%d.pth\" % (model, dataset, epoch))\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**orange2apple or apple2orange with fdcgan or resnet (improved fdgcan):**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset == \"orange2apple64\" or dataset == \"apple2orange64\") and (model == \"fdcgan\" or model == \"resnet\"):\n",
    "    prev_time = time.time()\n",
    "    for epoch in range(start_epoch, n_epochs):\n",
    "        for i, batch in enumerate(dataloader):\n",
    "\n",
    "            # Set model input\n",
    "            real_A = Variable(batch[my_class_A].type(Tensor))\n",
    "            real_B = Variable(batch[my_class_B].type(Tensor))\n",
    "\n",
    "            # Adversarial ground truths\n",
    "            valid = Variable(Tensor(np.ones((real_A.size(0), *discriminator.output_shape))), requires_grad=False)\n",
    "            fake = Variable(Tensor(np.zeros((real_A.size(0), *discriminator.output_shape))), requires_grad=False)\n",
    "\n",
    "            # ------------------\n",
    "            #  Train Generators\n",
    "            # ------------------\n",
    "\n",
    "            generator.train()\n",
    "\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            # Identity loss\n",
    "            loss_identity = identity_loss(generator(real_B), real_B) # Should't modify real_B\n",
    "\n",
    "            # GAN loss\n",
    "            fake_B = generator(real_A)\n",
    "            loss_GAN = adversarial_loss(discriminator(fake_B), valid)\n",
    "\n",
    "\n",
    "            # Total loss\n",
    "            loss_G = loss_GAN + lambda_id * loss_identity\n",
    "\n",
    "            loss_G.backward()\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            optimizer_D.zero_grad()\n",
    "\n",
    "            # Real loss\n",
    "            loss_real = adversarial_loss(discriminator(real_B), valid)\n",
    "            # TODO add using previous batch ?\n",
    "            loss_fake = adversarial_loss(discriminator(fake_B.detach()), fake)\n",
    "            # Total loss\n",
    "            loss_D = (loss_real + loss_fake) / 2\n",
    "\n",
    "            loss_D.backward()\n",
    "            optimizer_D.step()\n",
    "\n",
    "            # --------------\n",
    "            #  Log Progress\n",
    "            # --------------\n",
    "\n",
    "            # # Determine approximate time left\n",
    "            batches_done = epoch * len(dataloader) + i\n",
    "            batches_left = n_epochs * len(dataloader) - batches_done\n",
    "            time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
    "            prev_time = time.time()\n",
    "\n",
    "            # Print log\n",
    "            sys.stdout.write(\n",
    "                \"\\r[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f, adv: %f, identity: %f] ETA: %s\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    n_epochs,\n",
    "                    i,\n",
    "                    len(dataloader),\n",
    "                    loss_D.item(),\n",
    "                    loss_G.item(),\n",
    "                    loss_GAN.item(),\n",
    "                    loss_identity.item(),\n",
    "                    time_left,\n",
    "                )\n",
    "            )\n",
    "\n",
    "            # If at sample interval save image\n",
    "            if batches_done % sample_interval == 0:\n",
    "                sample_images(batches_done, my_class_A)\n",
    "\n",
    "        # Update learning rates\n",
    "        lr_scheduler_G.step()\n",
    "        lr_scheduler_D.step()\n",
    "\n",
    "        if checkpoint_interval != -1 and epoch % checkpoint_interval == 0:\n",
    "            # Save model checkpoints\n",
    "            torch.save(generator.state_dict(), \"saved_models/%s/%s/generator_%d.pth\" % (model, dataset, epoch))\n",
    "            torch.save(discriminator.state_dict(), \"saved_models/%s/%s/generator_%d.pth\" % (model, dataset, epoch))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GAN_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
